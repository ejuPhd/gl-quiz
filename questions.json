{
    "questions": [
        {
            "id": 1,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_basics",
            "difficulty": "beginner",
            "question": "Which type of machine learning requires a training dataset that has been labeled with the correct output?",
            "options": [
                "Unsupervised Learning",
                "Reinforcement Learning",
                "Supervised Learning",
                "Semi-Supervised Learning"
            ],
            "correct_answer": 2,
            "explanation": "Supervised learning algorithms are trained on data labeled with the correct output."
        },
        {
            "id": 2,
            "type": "multiple_choice",
            "category": "model_types_basics",
            "difficulty": "beginner",
            "question": "What is the primary output for a **Regression** problem?",
            "options": [
                "A discrete category or label",
                "A continuous value",
                "A cluster of data points",
                "A sequence of actions"
            ],
            "correct_answer": 1,
            "explanation": "Regression algorithms predict a continuous value based on the input variables, such as age or price."
        },
        {
            "id": 3,
            "type": "multiple_choice",
            "category": "ensemble_basics",
            "difficulty": "beginner",
            "question": "Which machine learning tool is described as a 'simple, decision-making diagram'?",
            "options": [
                "Random Forest",
                "Decision Tree",
                "Gradient Boosting Machine",
                "Support Vector Machine"
            ],
            "correct_answer": 1,
            "explanation": "A decision tree is a simple, decision making-diagram."
        },
        {
            "id": 4,
            "type": "multiple_choice",
            "category": "error_basics",
            "difficulty": "beginner",
            "question": "What situation is called 'high bias' and occurs when a model is too simple for the data?",
            "options": [
                "Overfitting",
                "Underfitting",
                "High Variance",
                "Optimal Solution"
            ],
            "correct_answer": 1,
            "explanation": "Underfitting is a situation when your model is too simple for your data, which is also called high bias."
        },
        {
            "id": 5,
            "type": "multiple_choice",
            "category": "loss_function_basics",
            "difficulty": "beginner",
            "question": "In a Regression context, what does a loss function intuitively represent?",
            "options": [
                "The model's accuracy",
                "The time taken to train the model",
                "A 'cost' associated with the event/error",
                "The complexity of the model"
            ],
            "correct_answer": 2,
            "explanation": "A loss function maps an event or values onto a real number intuitively representing some 'cost' associated with the event/error."
        },
        {
            "id": 6,
            "type": "multiple_choice",
            "category": "supervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which of the following is listed as a supervised learning algorithm?",
            "options": [
                "K-means Clustering",
                "Principal Component Analysis (PCA)",
                "Decision Trees",
                "Gaussian Mixture Models"
            ],
            "correct_answer": 2,
            "explanation": "Decision Trees are listed as an algorithm under Supervised Learning."
        },
        {
            "id": 7,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which of the following is an example of an unsupervised learning algorithm?",
            "options": [
                "Linear Regression",
                "Random Forest",
                "K-means Clustering",
                "Support Vector Machines"
            ],
            "correct_answer": 2,
            "explanation": "K-means Clustering is listed as an unsupervised learning algorithm."
        },
        {
            "id": 8,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "beginner",
            "question": "Which problem is a significant disadvantage of simple decision trees, where the result can change largely based on tiny changes to the training data?",
            "options": [
                "Bias error",
                "Overfitting",
                "Low Variance",
                "Variance error"
            ],
            "correct_answer": 3,
            "explanation": "Variance error refers to how much a result will change based on changes to the training set. Decision trees have high variance."
        },
        {
            "id": 9,
            "type": "multiple_choice",
            "category": "loss_function_types",
            "difficulty": "beginner",
            "question": "Mean Absolute Error (MAE) is also known by which other loss name?",
            "options": [
                "L2 loss",
                "L1 loss",
                "Huber loss",
                "Quantile loss"
            ],
            "correct_answer": 1,
            "explanation": "Mean Absolute Error (MAE) is also known as the L1 loss."
        },
        {
            "id": 10,
            "type": "multiple_choice",
            "category": "loss_function_types",
            "difficulty": "beginner",
            "question": "Mean Squared Error (MSE) is also known by which other loss name?",
            "options": [
                "L1 loss",
                "L2 loss",
                "Log Cosh loss",
                "Relative Absolute Error"
            ],
            "correct_answer": 1,
            "explanation": "Mean Squared Error (MSE) is also known as the L2 loss."
        },
        {
            "id": 11,
            "type": "multiple_choice",
            "category": "unsupervised_learning_examples",
            "difficulty": "beginner",
            "question": "Which of these is a common real-world application for **unsupervised** learning?",
            "options": [
                "Spam Email Filter",
                "Credit Scoring",
                "Image Recognition",
                "Customer Segmentation"
            ],
            "correct_answer": 3,
            "explanation": "Customer Segmentation is listed as an example of unsupervised learning."
        },
        {
            "id": 12,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "What is the general strategy to fix **underfitting**?",
            "options": [
                "Simplify the model",
                "Complicate the model",
                "Use more regularization",
                "Reduce the quantity of features"
            ],
            "correct_answer": 1,
            "explanation": "To fix underfitting, you should complicate the model."
        },
        {
            "id": 13,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "What is the general strategy to fix **overfitting**?",
            "options": [
                "Complicate the model",
                "Simplify the model",
                "Decrease regularization",
                "Use a larger quantity of features"
            ],
            "correct_answer": 1,
            "explanation": "To fix overfitting, you should simplify the model."
        },
        {
            "id": 14,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "beginner",
            "question": "Overfitting in a decision tree is possible with one large (deep) tree due to factors like the presence of what?",
            "options": [
                "Small number of features",
                "Lack of representative instances and noise",
                "Low variance in the data",
                "High bias in the data"
            ],
            "correct_answer": 1,
            "explanation": "Overfitting happens for many reasons, including the presence of noise and lack of representative instances. It's possible for overfitting with one large (deep) tree."
        },
        {
            "id": 15,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "The goal of Supervised Learning is **Prediction**. What is the goal of Unsupervised Learning?",
            "options": [
                "Generalization",
                "Pattern discovery",
                "Classification",
                "Regression"
            ],
            "correct_answer": 1,
            "explanation": "The goal of Supervised Learning is Prediction, while the goal of Unsupervised Learning is Pattern discovery."
        },
        {
            "id": 16,
            "type": "multiple_choice",
            "category": "loss_function_formulas",
            "difficulty": "beginner",
            "question": "Which loss function measures the ability of the model to overestimate or underestimate the bias, without using the absolute value?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Mean Bias Error (MBE)",
                "Root Mean Squared Error (RMSE)"
            ],
            "correct_answer": 2,
            "explanation": "Mean Bias Error (MBE) measures the bias and differs from MAE as it doesn't use the absolute value."
        },
        {
            "id": 17,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "beginner",
            "question": "When detecting underfitting, which condition is true for the training error?",
            "options": [
                "Train error is very small",
                "Train error is large",
                "Train error is small, but larger than in overfitting",
                "Train error is zero"
            ],
            "correct_answer": 1,
            "explanation": "In underfitting, the train error is large and val/test error is large too."
        },
        {
            "id": 18,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "beginner",
            "question": "When detecting overfitting, which condition is true for the training error?",
            "options": [
                "Train error is large",
                "Train error is very small",
                "Train error is similar to test error",
                "Train error is small, but larger than in a good model"
            ],
            "correct_answer": 1,
            "explanation": "In overfitting, the train error is very small and val/test error is large."
        },
        {
            "id": 19,
            "type": "multiple_choice",
            "category": "supervised_learning_examples",
            "difficulty": "beginner",
            "question": "A spam email filter is an example of which type of machine learning application?",
            "options": [
                "Unsupervised Learning",
                "Reinforcement Learning",
                "Supervised Learning",
                "Clustering"
            ],
            "correct_answer": 2,
            "explanation": "A Spam Email Filter is listed as an example of Supervised Learning."
        },
        {
            "id": 20,
            "type": "multiple_choice",
            "category": "random_forest_basics",
            "difficulty": "beginner",
            "question": "What term refers to a 'collection of decision trees with a single, aggregated result'?",
            "options": [
                "Decision Tree",
                "Gradient Boosting Machine",
                "Random Forest",
                "Linear Regression"
            ],
            "correct_answer": 2,
            "explanation": "A random forest is a collection of decision trees with a single, aggregated result."
        },
        {
            "id": 21,
            "type": "multiple_choice",
            "category": "loss_function_formulas",
            "difficulty": "beginner",
            "question": "Which regression loss function uses the square root of Mean Squared Error (MSE)?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Relative Absolute Error (RAE)",
                "Root Mean Squared Error (RMSE)",
                "Mean Absolute Percentage Error (MAPE)"
            ],
            "correct_answer": 2,
            "explanation": "The square root of MSE is used to calculate RMSE."
        },
        {
            "id": 22,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "Compared to unsupervised learning, supervised learning is generally considered:",
            "options": [
                "More complex",
                "Less accurate",
                "Harder to interpret",
                "Generally simpler"
            ],
            "correct_answer": 3,
            "explanation": "Supervised learning is generally simpler compared to unsupervised learning's 'Can be more complex'."
        },
        {
            "id": 23,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "beginner",
            "question": "What is the result when the validation error is big, and the training error is small?",
            "options": [
                "Underfitting",
                "Overfitting",
                "Good model",
                "Train and test sets are different"
            ],
            "correct_answer": 1,
            "explanation": "If train error is small and validation error is big, the result is Overfitting."
        },
        {
            "id": 24,
            "type": "multiple_choice",
            "category": "model_complexity_basics",
            "difficulty": "beginner",
            "question": "The concept of 'high variance' is associated with which issue?",
            "options": [
                "Underfitting",
                "Low Bias",
                "Optimal Solution",
                "Overfitting"
            ],
            "correct_answer": 3,
            "explanation": "Overfitting is a situation when your model is too complex for your data, which is also called high variance."
        },
        {
            "id": 25,
            "type": "multiple_choice",
            "category": "random_forest_characteristics",
            "difficulty": "beginner",
            "question": "Compared to a single decision tree, Random Forests are commonly reported as what?",
            "options": [
                "Slower to build",
                "Less accurate",
                "The most accurate learning algorithm",
                "Easier to interpret"
            ],
            "correct_answer": 2,
            "explanation": "Random forests are commonly reported as the most accurate learning algorithm."
        },
        {
            "id": 26,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "What key difference exists between supervised and unsupervised learning algorithms regarding data?",
            "options": [
                "Supervised uses continuous data, unsupervised uses discrete data",
                "Supervised requires labeled data, unsupervised works with unlabeled data",
                "Supervised is for classification only, unsupervised is for regression only",
                "Supervised uses small datasets, unsupervised uses large datasets"
            ],
            "correct_answer": 1,
            "explanation": "Supervised learning requires labeled data, while unsupervised learning works with unlabeled data."
        },
        {
            "id": 27,
            "type": "multiple_choice",
            "category": "unsupervised_learning_examples",
            "difficulty": "beginner",
            "question": "Anomaly Detection in network traffic is a practical example of which learning type?",
            "options": [
                "Supervised Learning",
                "Semi-Supervised Learning",
                "Unsupervised Learning",
                "Reinforcement Learning"
            ],
            "correct_answer": 2,
            "explanation": "Anomaly Detection is an example of Unsupervised Learning."
        },
        {
            "id": 28,
            "type": "multiple_choice",
            "category": "supervised_learning_examples",
            "difficulty": "beginner",
            "question": "Predicting the creditworthiness of a customer is an example of which type of machine learning application?",
            "options": [
                "Unsupervised Learning",
                "Supervised Learning",
                "Clustering",
                "Dimensionality Reduction"
            ],
            "correct_answer": 1,
            "explanation": "Credit Scoring is listed as an example of Supervised Learning."
        },
        {
            "id": 29,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "To complicate a model, you generally need to add more of what?",
            "options": [
                "Regularization terms",
                "Outliers",
                "Training data noise",
                "Parameters (degrees of freedom)"
            ],
            "correct_answer": 3,
            "explanation": "To complicate the model, you need to add more parameters (degrees of freedom)."
        },
        {
            "id": 30,
            "type": "multiple_choice",
            "category": "regression_vs_classification",
            "difficulty": "beginner",
            "question": "Classification is a predictive model that approximates a mapping function to identify what kind of output variables?",
            "options": [
                "Continuous",
                "Discrete (labels or categories)",
                "Numeric (prices, ages)",
                "Unlabeled"
            ],
            "correct_answer": 1,
            "explanation": "Classification is a predictive model... to identify discrete output variables, which can be labels or categories."
        },
        {
            "id": 31,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "Which type of learning is analogous to learning to cook with a recipe book, where you know the end dish and have step-by-step instructions (labels)?",
            "options": [
                "Unsupervised Learning",
                "Semi-Supervised Learning",
                "Reinforcement Learning",
                "Supervised Learning"
            ],
            "correct_answer": 3,
            "explanation": "Supervised Learning is like learning to cook with a recipe book, with step-by-step instructions (labels)."
        },
        {
            "id": 32,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "Which type of learning is good for **exploratory analysis** to find hidden patterns, as it can work with unlabeled data?",
            "options": [
                "Supervised Learning",
                "Semi-Supervised Learning",
                "Reinforcement Learning",
                "Unsupervised Learning"
            ],
            "correct_answer": 3,
            "explanation": "Unsupervised learning is good for exploratory analysis to find hidden patterns."
        },
        {
            "id": 33,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "beginner",
            "question": "Which common Regression loss function is **not** sensitive to outliers but is **not** differentiable at zero?",
            "options": [
                "Mean Squared Error (MSE)",
                "Root Mean Squared Error (RMSE)",
                "Mean Absolute Error (MAE)",
                "Relative Squared Error (RSE)"
            ],
            "correct_answer": 2,
            "explanation": "Mean Absolute Error (MAE) is not sensitive to outliers and is also not differentiable at zero."
        },
        {
            "id": 34,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "beginner",
            "question": "Which common Regression loss function handles outliers efficiently due to the quadratic loss?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Mean Bias Error (MBE)",
                "Quantile Loss"
            ],
            "correct_answer": 1,
            "explanation": "Mean Squared Error (MSE) handles outliers in an efficient manner as outliers are detected due to the quadratic loss."
        },
        {
            "id": 35,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "beginner",
            "question": "Error due to **bias** happens when you place too many what on target functions?",
            "options": [
                "Samples",
                "Features",
                "Predictions",
                "Restrictions"
            ],
            "correct_answer": 3,
            "explanation": "Bias error happens when you place too many restrictions on target functions."
        },
        {
            "id": 36,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "beginner",
            "question": "What is the key difference in how Random Forests and Gradient Boosting Machines (GBM) start the process of combining decision trees?",
            "options": [
                "RF combines at the beginning, GBM combines at the end.",
                "RF combines sequentially, GBM combines independently.",
                "RF combines at the end, GBM starts combining at the beginning.",
                "They both combine trees at the end."
            ],
            "correct_answer": 2,
            "explanation": "Random forests combine at the end; Gradient boosting machines also combine decision trees, but start the combining process at the beginning."
        },
        {
            "id": 37,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which unsupervised learning algorithm is used to reduce the dimensionality of data?",
            "options": [
                "K-means Clustering",
                "Principal Component Analysis (PCA)",
                "Hierarchical Clustering",
                "Gaussian Mixture Models"
            ],
            "correct_answer": 1,
            "explanation": "Principal Component Analysis (PCA) reduces the dimensionality of the data."
        },
        {
            "id": 38,
            "type": "multiple_choice",
            "category": "random_forest_features",
            "difficulty": "beginner",
            "question": "In a Random Forest, building and combining what size of trees helps reduce the variance seen in decision trees?",
            "options": [
                "Large (deep) trees",
                "Medium-sized trees",
                "Small (shallow) trees",
                "Any size of trees"
            ],
            "correct_answer": 2,
            "explanation": "Random forests reduce the variance... by building and combining small (shallow) trees."
        },
        {
            "id": 39,
            "type": "multiple_choice",
            "category": "decision_tree_characteristics",
            "difficulty": "beginner",
            "question": "A single decision tree is considered a what kind of predictor, but is relatively fast to build?",
            "options": [
                "Strong",
                "Robust",
                "Weak",
                "Optimized"
            ],
            "correct_answer": 2,
            "explanation": "A single decision tree is a weak predictor, but is relatively fast to build."
        },
        {
            "id": 40,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "Adding more trees to a Random Forest gives you a more robust model and helps prevent what?",
            "options": [
                "Underfitting",
                "Low variance",
                "Overfitting",
                "Bias error"
            ],
            "correct_answer": 2,
            "explanation": "More trees give you a more robust model and prevent overfitting."
        },
        {
            "id": 41,
            "type": "multiple_choice",
            "category": "unsupervised_learning_examples",
            "difficulty": "beginner",
            "question": "Market basket analysis, identifying products often purchased together, is an example of which learning type?",
            "options": [
                "Supervised Classification",
                "Supervised Regression",
                "Unsupervised Learning",
                "Reinforcement Learning"
            ],
            "correct_answer": 2,
            "explanation": "Market basket analysis... is an example of Unsupervised Learning."
        },
        {
            "id": 42,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "beginner",
            "question": "A good model is characterized by what condition regarding its train and val/test errors?",
            "options": [
                "Both train and val/test errors are large.",
                "Train error is very small, and val/test error is large.",
                "Both train and val/test errors are small, with the train error being slightly larger than in overfitting.",
                "Train error is zero, and val/test error is zero."
            ],
            "correct_answer": 2,
            "explanation": "When you find a good model, train error is small (but larger than in the case of overfitting), and val/test error is small too."
        },
        {
            "id": 43,
            "type": "multiple_choice",
            "category": "decision_tree_characteristics",
            "difficulty": "beginner",
            "question": "Compared to a Random Forest, a single Decision Tree is generally considered what to read or interpret?",
            "options": [
                "More complicated",
                "Easier",
                "Equally complicated",
                "Steeper learning curve"
            ],
            "correct_answer": 1,
            "explanation": "A decision tree is easy to read... while a random forest is a tad more complicated to interpret."
        },
        {
            "id": 44,
            "type": "multiple_choice",
            "category": "loss_function_formulas",
            "difficulty": "beginner",
            "question": "Which regression loss function is a ratio-based metric, with possible values between 0 and infinity, where zero is the best value?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Relative Absolute Error (RAE)",
                "Mean Bias Error (MBE)"
            ],
            "correct_answer": 2,
            "explanation": "Relative Absolute Error (RAE) is a ratio-based metric... RAE has a possible value between 0 and infinity, with zero being the best value."
        },
        {
            "id": 45,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which unsupervised algorithm is a tree of clusters that is built up from the data?",
            "options": [
                "K-means Clustering",
                "Principal Component Analysis (PCA)",
                "Gaussian Mixture Models",
                "Hierarchical Clustering"
            ],
            "correct_answer": 3,
            "explanation": "Hierarchical Clustering builds a tree of clusters."
        },
        {
            "id": 46,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which unsupervised learning algorithm divides data into non-overlapping subsets?",
            "options": [
                "Hierarchical Clustering",
                "Principal Component Analysis (PCA)",
                "K-means Clustering",
                "t-SNE"
            ],
            "correct_answer": 2,
            "explanation": "K-means Clustering divides data into non-overlapping subsets."
        },
        {
            "id": 47,
            "type": "multiple_choice",
            "category": "supervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which supervised learning algorithm is an ensemble of decision trees?",
            "options": [
                "Linear Regression",
                "Support Vector Machines",
                "Random Forest",
                "Neural Networks"
            ],
            "correct_answer": 2,
            "explanation": "Random Forest is an ensemble of decision trees."
        },
        {
            "id": 48,
            "type": "multiple_choice",
            "category": "supervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which supervised learning algorithm predicts a continuous outcome variable?",
            "options": [
                "Linear Regression",
                "Support Vector Machines (SVM)",
                "Decision Trees (Classification)",
                "Random Forest (Classification)"
            ],
            "correct_answer": 0,
            "explanation": "Linear Regression predicts a continuous outcome variable based on one or more predictor variables."
        },
        {
            "id": 49,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which unsupervised algorithm is used for dimensionality reduction and visualization?",
            "options": [
                "K-means Clustering",
                "Gaussian Mixture Models",
                "Hierarchical Clustering",
                "t-SNE"
            ],
            "correct_answer": 3,
            "explanation": "t-SNE is used for dimensionality reduction and visualization."
        },
        {
            "id": 50,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "beginner",
            "question": "Which loss function is calculated by dividing the total absolute error by the absolute difference between the mean and the actual value?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Relative Absolute Error (RAE)",
                "Relative Squared Error (RSE)"
            ],
            "correct_answer": 2,
            "explanation": "The calculation of Relative Absolute Error (RAE) involves taking the total absolute error and dividing it by the absolute difference between the mean and the actual value."
        },
        {
            "id": 51,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "Supervised Learning requires what kind of data, which can be time-consuming and expensive to obtain?",
            "options": [
                "Unlabeled data",
                "Large data",
                "Labeled data",
                "Continuous data"
            ],
            "correct_answer": 2,
            "explanation": "Supervised learning requires labeled data, which can be time-consuming and expensive to obtain."
        },
        {
            "id": 52,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "When is Supervised Learning recommended for use?",
            "options": [
                "When you want the algorithm to find natural structures in the data.",
                "When you're not sure what you're looking for.",
                "When you have a clear goal of predicting a specific outcome and have labeled data.",
                "When results are harder to interpret."
            ],
            "correct_answer": 2,
            "explanation": "Supervised Learning is recommended when you have a clear goal of predicting a specific outcome and have labeled data to train on."
        },
        {
            "id": 53,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "beginner",
            "question": "When is Unsupervised Learning recommended for use?",
            "options": [
                "When you have a clear goal of predicting a specific outcome.",
                "When you're not sure what you're looking for and want the algorithm to find natural structures.",
                "When you have abundant labeled data.",
                "When you want high accuracy and reliability for a known task."
            ],
            "correct_answer": 1,
            "explanation": "Unsupervised Learning is recommended when you're not sure what you're looking for and want the algorithm to find natural structures in the data."
        },
        {
            "id": 54,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "beginner",
            "question": "What kind of error happens when the result is restricted by a simple binary algorithm (like true/false choices)?",
            "options": [
                "Variance error",
                "Overfitting",
                "Bias error",
                "Low variance"
            ],
            "correct_answer": 2,
            "explanation": "Bias error happens when you place too many restrictions on target functions, for example, by a simple binary algorithm."
        },
        {
            "id": 55,
            "type": "multiple_choice",
            "category": "supervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Support Vector Machines (SVM) are mainly used for what type of problems?",
            "options": [
                "Regression problems",
                "Dimensionality Reduction problems",
                "Classification problems",
                "Clustering problems"
            ],
            "correct_answer": 2,
            "explanation": "Support Vector Machines are mainly used for classification problems."
        },
        {
            "id": 56,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which unsupervised learning algorithm assumes that data is generated from a mixture of several Gaussian distributions?",
            "options": [
                "K-means Clustering",
                "Hierarchical Clustering",
                "Principal Component Analysis (PCA)",
                "Gaussian Mixture Models"
            ],
            "correct_answer": 3,
            "explanation": "Gaussian Mixture Models assumes that data is generated from a mixture of several Gaussian distributions."
        },
        {
            "id": 57,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "beginner",
            "question": "If the validation and test errors are very different, what might you need to do?",
            "options": [
                "Simplify the model (overfitting).",
                "Complicate the model (underfitting).",
                "Get more data similar to test data and check the split.",
                "Decrease regularization."
            ],
            "correct_answer": 2,
            "explanation": "If validation and test error are very different, then you need to get more data similar to test data and make sure that you split the data correctly."
        },
        {
            "id": 58,
            "type": "multiple_choice",
            "category": "loss_function_formulas",
            "difficulty": "beginner",
            "question": "In the formula for Mean Absolute Error (MAE), the term for the predicted value (often written as y-hat) represents what value?",
            "options": [
                "The actual (true) value",
                "The mean value",
                "The predicted value",
                "The total number of samples"
            ],
            "correct_answer": 2,
            "explanation": "MAE measures the absolute difference between the true value and the predicted value. The predicted value is the output of the model."
        },
        {
            "id": 59,
            "type": "multiple_choice",
            "category": "loss_function_formulas",
            "difficulty": "beginner",
            "question": "In the formula for Mean Squared Error (MSE), the term y_i represents what value?",
            "options": [
                "The actual (true) value",
                "The predicted value",
                "The total number of samples",
                "The squared error"
            ],
            "correct_answer": 0,
            "explanation": "MSE calculates the squared difference between the actual value (y_i) and the predicted value (y-hat)."
        },
        {
            "id": 60,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "beginner",
            "question": "Gradient Boosting Machines introduce a **weak learner** to improve the shortcomings of existing **weak learners** in what kind of manner?",
            "options": [
                "Parallel stage-wise manner",
                "Random stage-wise manner",
                "Forward stage-wise manner",
                "End-of-process averaging"
            ],
            "correct_answer": 2,
            "explanation": "This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners."
        },
        {
            "id": 61,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "To simplify a model, you need to what regarding the number of parameters?",
            "options": [
                "Increase the number of parameters",
                "Keep the number of parameters the same",
                "Introduce new parameters",
                "Reduce the number of parameters"
            ],
            "correct_answer": 3,
            "explanation": "To simplify the model, you need contrariwise to reduce the number of parameters."
        },
        {
            "id": 62,
            "type": "multiple_choice",
            "category": "regularization_basics",
            "difficulty": "beginner",
            "question": "What is the effect of the regularization term in a model's complexity?",
            "options": [
                "Complicates the model directly",
                "Requires the model to use larger parameter values",
                "Is an indirect and forced simplification of the model",
                "Has no effect on model complexity"
            ],
            "correct_answer": 2,
            "explanation": "Regularization is an indirect and forced simplification of the model. The regularization term requires the model to keep parameter values as small as possible, so requires the model to be as simple as possible."
        },
        {
            "id": 63,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "Adding new 'natural' features (obtaining new features for existing data) is a way to do what to the model?",
            "options": [
                "Simplify",
                "Complicate",
                "Regularize",
                "Reduce variance"
            ],
            "correct_answer": 1,
            "explanation": "Adding new features also complicates the model."
        },
        {
            "id": 64,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "beginner",
            "question": "Relative Squared Error (RSE) normalizes the total squared error by dividing it by the total squared error of what simple predictor?",
            "options": [
                "Zero value",
                "The predicted value",
                "The average of the actual values",
                "The maximum actual value"
            ],
            "correct_answer": 2,
            "explanation": "The relative squared error takes the total squared error and normalizes it by dividing by the total squared error of the simple predictor, which is just the average of the actual values."
        },
        {
            "id": 65,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "beginner",
            "question": "Which type of learning is analogous to being in a kitchen without a recipe book, experimenting to discover new dishes and the relationships between ingredients?",
            "options": [
                "Supervised Learning",
                "Reinforcement Learning",
                "Semi-Supervised Learning",
                "Unsupervised Learning"
            ],
            "correct_answer": 3,
            "explanation": "Unsupervised Learning is like being in a kitchen without a recipe book, experimenting with ingredients and cooking methods to discover new dishes and the relationships between ingredients."
        },
        {
            "id": 66,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "beginner",
            "question": "In Mean Squared Error (MSE), convergence is generally smooth because as the loss decreases, what happens to the gradient?",
            "options": [
                "It becomes larger",
                "It becomes smaller",
                "It remains constant",
                "It becomes zero"
            ],
            "correct_answer": 1,
            "explanation": "Convergence is also smooth as the gradient becomes smaller as the loss decreases (for MSE)."
        },
        {
            "id": 67,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "beginner",
            "question": "What does the symbol $\\overline{y}$ represent in the Relative Absolute Error (RAE) and Relative Squared Error (RSE) formulas?",
            "options": [
                "The predicted value",
                "The total number of samples",
                "The maximum observed value",
                "The average of the actual values"
            ],
            "correct_answer": 3,
            "explanation": "$\\overline{y}$ is defined as the mean (average) of the actual values ($y_{i}$) in the provided formulas."
        },
        {
            "id": 68,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "Using a smaller quantity of features is a technique to fight which problem?",
            "options": [
                "Underfitting",
                "Low bias",
                "Overfitting",
                "Low variance"
            ],
            "correct_answer": 2,
            "explanation": "A smaller quantity of features is a technique to fight overfitting."
        },
        {
            "id": 69,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "Using a larger quantity of features is a technique to fight which problem?",
            "options": [
                "Overfitting",
                "High variance",
                "Underfitting",
                "Optimal solution"
            ],
            "correct_answer": 2,
            "explanation": "A larger quantity of features is a technique to fight underfitting."
        },
        {
            "id": 70,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "Using more regularization is a technique to fight which problem?",
            "options": [
                "Underfitting",
                "High bias",
                "Overfitting",
                "Low variance"
            ],
            "correct_answer": 2,
            "explanation": "More regularization is a technique to fight overfitting."
        },
        {
            "id": 71,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "Using less regularization is a technique to fight which problem?",
            "options": [
                "Overfitting",
                "High variance",
                "Underfitting",
                "Optimal solution"
            ],
            "correct_answer": 2,
            "explanation": "Less regularization is a technique to fight underfitting."
        },
        {
            "id": 72,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "For a very simple model (Underfitting), what is the effect of getting more data?",
            "options": [
                "It will help significantly.",
                "It will not help.",
                "It will cause overfitting.",
                "It will increase the variance error."
            ],
            "correct_answer": 1,
            "explanation": "Getting more data will not help in the case of underfitting."
        },
        {
            "id": 73,
            "type": "multiple_choice",
            "category": "model_improvement_basics",
            "difficulty": "beginner",
            "question": "For a model that is Overfitting, what is the potential effect of getting more data?",
            "options": [
                "It will not help at all.",
                "It will most likely help.",
                "It will cause underfitting.",
                "It will increase the bias error."
            ],
            "correct_answer": 1,
            "explanation": "Getting more data can help with overfitting, assuming the model is not TOO complex."
        },
        {
            "id": 74,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "beginner",
            "question": "The Mean Absolute Percentage Error (MAPE) is a common loss function that expresses the error in what units?",
            "options": [
                "A decimal value",
                "An absolute value",
                "A percentage",
                "A squared value"
            ],
            "correct_answer": 2,
            "explanation": "The MAPE formula includes a multiplication by 100%, which converts the error to a percentage."
        },
        {
            "id": 75,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "beginner",
            "question": "What is the result for the Relative Squared Error (RSE) loss if the model is a perfect fit?",
            "options": [
                "Infinity",
                "100",
                "0",
                "The average of the actual values"
            ],
            "correct_answer": 2,
            "explanation": "For a perfect fit, the numerator is equal to 0 and loss is 0 for RSE."
        },
        {
            "id": 76,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "intermediate",
            "question": "What is the key difference in how Random Forests and Gradient Boosting Machines (GBM) build their trees?",
            "options": [
                "RF builds one tree at a time, while GBM builds each tree independently.",
                "RF builds trees independently, while GBM builds one tree at a time.",
                "RF uses deep trees, while GBM uses shallow trees.",
                "RF uses averages, while GBM uses majority rules."
            ],
            "correct_answer": 1,
            "explanation": "Random Forests builds each tree independently, while Gradient Boosting builds one tree at a time."
        },
        {
            "id": 77,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "intermediate",
            "question": "In the context of neural networks, which of the following is listed as a main regularization method?",
            "options": [
                "Feature Selection",
                "Early Stopping",
                "Feature Engineering",
                "Polynomial Features"
            ],
            "correct_answer": 1,
            "explanation": "In the context of neural networks, the main regularization methods are Early stopping, Dropout, and L1 and L2 Regularization."
        },
        {
            "id": 78,
            "type": "multiple_choice",
            "category": "unsupervised_learning_characteristics",
            "difficulty": "intermediate",
            "question": "A major **con** of unsupervised learning compared to supervised learning is that the results can be:",
            "options": [
                "More accurate",
                "Easier to interpret",
                "Less versatile",
                "Harder to interpret"
            ],
            "correct_answer": 3,
            "explanation": "A con of unsupervised learning is that results can be harder to interpret."
        },
        {
            "id": 79,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "intermediate",
            "question": "Error due to **bias** is exemplified by restricting the result with a function like a what, when the true relationship is more complex?",
            "options": [
                "High-degree polynomial",
                "Linear equation",
                "Deep neural network",
                "Random Forest"
            ],
            "correct_answer": 1,
            "explanation": "Restricting your result with a restricting function (e.g., a linear equation) will often result in bias."
        },
        {
            "id": 80,
            "type": "multiple_choice",
            "category": "random_forest_tradeoffs",
            "difficulty": "intermediate",
            "question": "What is a major trade-off when adding more trees to a Random Forest?",
            "options": [
                "It becomes more susceptible to underfitting.",
                "The model becomes more simple.",
                "The process becomes slower.",
                "The model becomes easier to interpret."
            ],
            "correct_answer": 2,
            "explanation": "The more trees you have, the slower the process."
        },
        {
            "id": 81,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "Which two regression loss functions are combined in the Huber Loss function?",
            "options": [
                "MAE and MBE",
                "RAE and RSE",
                "Quadratic and Linear scoring algorithms",
                "MSE and MSLE"
            ],
            "correct_answer": 2,
            "explanation": "Huber loss is an ideal combination of quadratic and linear scoring algorithms."
        },
        {
            "id": 82,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Due to the loss being log, Mean Squared Logarithmic Error (MSLE) and Root Mean Squared Logarithmic Error (RMSLE) penalize which type of errors more heavily?",
            "options": [
                "Outliers",
                "Overestimates",
                "Underestimates",
                "Errors close to zero"
            ],
            "correct_answer": 2,
            "explanation": "Due to the loss being log it penalizes underestimates more than overestimates."
        },
        {
            "id": 83,
            "type": "multiple_choice",
            "category": "model_complexity_adjustment",
            "difficulty": "intermediate",
            "question": "To complicate an ensemble model like Gradient Boosting, what action is specifically mentioned in the documents?",
            "options": [
                "Decrease the number of models in boosting",
                "Decrease the maximum tree depth",
                "Increase the number of models in boosting",
                "Increase regularization"
            ],
            "correct_answer": 2,
            "explanation": "If the algorithm is already quite complex... you need to add more parameters to it, for example, increase the number of models in boosting."
        },
        {
            "id": 84,
            "type": "multiple_choice",
            "category": "feature_engineering",
            "difficulty": "intermediate",
            "question": "In the context of polynomial regression, adding quadratic features to a dataset allows a linear model to do what?",
            "options": [
                "Be simplified",
                "Capture quadratic data",
                "Become more susceptible to underfitting",
                "Decrease its variance"
            ],
            "correct_answer": 1,
            "explanation": "Adding new features... complicates the model... adding quadratic features to a dataset allows a linear model to recover quadratic data."
        },
        {
            "id": 85,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "A potential issue with Mean Bias Error (MBE) is that some errors may cancel out between individual samples, resulting in what misleading outcome?",
            "options": [
                "High variance",
                "An underestimate of the true bias",
                "A large loss value",
                "A zero loss"
            ],
            "correct_answer": 3,
            "explanation": "The issue with this loss (MBE) is that some times the errors may cancel out between individual samples resulting in a zero loss which may be misleading."
        },
        {
            "id": 86,
            "type": "multiple_choice",
            "category": "random_forest_features",
            "difficulty": "intermediate",
            "question": "Besides using different samples for training and combining shallow trees, which technique do Random Forests use to reduce the variance seen in decision trees?",
            "options": [
                "Reducing the number of training samples",
                "Specifying random feature subsets",
                "Using a linear equation as the base model",
                "Employing high-degree polynomials"
            ],
            "correct_answer": 1,
            "explanation": "Random forests reduce the variance... by specifying random feature subsets."
        },
        {
            "id": 87,
            "type": "multiple_choice",
            "category": "bias_variance_tradeoff",
            "difficulty": "intermediate",
            "question": "In the Bias/Variance Trade-off, **low bias, high variance** corresponds to which model situation?",
            "options": [
                "Good result",
                "Underfitting",
                "Overfitting",
                "Very bad algorithm"
            ],
            "correct_answer": 2,
            "explanation": "Low bias, high variance corresponds to overfitting."
        },
        {
            "id": 88,
            "type": "multiple_choice",
            "category": "bias_variance_tradeoff",
            "difficulty": "intermediate",
            "question": "In the Bias/Variance Trade-off, **high bias, low variance** corresponds to which model situation?",
            "options": [
                "Good result",
                "Underfitting",
                "Overfitting",
                "Very bad algorithm"
            ],
            "correct_answer": 1,
            "explanation": "High bias, low variance corresponds to underfitting."
        },
        {
            "id": 89,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Mean Absolute Percentage Error (MAPE) is a metric primarily used to assess the accuracy of a what?",
            "options": [
                "Classification model",
                "Dimensionality reduction model",
                "Forecast system",
                "Clustering algorithm"
            ],
            "correct_answer": 2,
            "explanation": "The Mean Absolute Percentage Error (MAPE)... is a metric used to assess the accuracy of a forecast system."
        },
        {
            "id": 90,
            "type": "multiple_choice",
            "category": "feature_selection",
            "difficulty": "intermediate",
            "question": "To simplify a model, you should remove all additional features and use what approaches to select only useful features from the original dataset?",
            "options": [
                "Polynomial features",
                "Feature selection",
                "Feature augmentation",
                "Feature generation"
            ],
            "correct_answer": 1,
            "explanation": "To simplify the model... you need to use feature selection approaches to select only those features that carry the maximum amount of useful information."
        },
        {
            "id": 91,
            "type": "multiple_choice",
            "category": "regularization_parameters",
            "difficulty": "intermediate",
            "question": "To simplify a model (i.e., increase regularization impact) using a Decision Tree, what hyperparameter should you likely **decrease**?",
            "options": [
                "Number of samples",
                "Maximum tree depth",
                "Number of trees",
                "Learning rate"
            ],
            "correct_answer": 1,
            "explanation": "Maximum tree depth is an example of a parameter that influences regularization/complexity in decision trees. To simplify the model, you reduce the number of degrees of freedom, which means reducing the tree depth."
        },
        {
            "id": 92,
            "type": "multiple_choice",
            "category": "feature_engineering",
            "difficulty": "intermediate",
            "question": "What is the term for artificially obtaining additional features from existing ones (e.g., creating interaction terms)?",
            "options": [
                "Feature Selection",
                "Data Augmentation",
                "Feature Engineering",
                "Dimensionality Reduction"
            ],
            "correct_answer": 2,
            "explanation": "Artificial obtaining of additional features from existing ones (the so-called feature engineering) is used quite often for classical machine learning models."
        },
        {
            "id": 93,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "intermediate",
            "question": "What is a major **con** of supervised learning compared to unsupervised learning?",
            "options": [
                "Easier to understand and interpret",
                "High accuracy when labeled data is abundant",
                "Requires labeled data which can be time-consuming and expensive",
                "May not handle unseen data well if properly generalized"
            ],
            "correct_answer": 2,
            "explanation": "A con is that supervised learning requires labeled data, which can be time-consuming and expensive to obtain."
        },
        {
            "id": 94,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Which loss function's goal is to minimize the loss by finding the appropriate $\\gamma$ that balances positive and negative errors?",
            "options": [
                "Mean Squared Error (MSE)",
                "Mean Absolute Error (MAE)",
                "Huber Loss",
                "Quantile Loss"
            ],
            "correct_answer": 3,
            "explanation": "The Quantile regression loss function is used to forecast quantiles, and the quantile values (gamma) are chosen to balance the positive and the negative errors."
        },
        {
            "id": 95,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "Log Cosh Loss is calculated by taking the logarithm of the hyperbolic cosine of what value?",
            "options": [
                "The absolute error",
                "The predicted value",
                "The difference between actual and predicted values (error)",
                "The mean of the actual values"
            ],
            "correct_answer": 2,
            "explanation": "Log cosh calculates the logarithm of the hyperbolic cosine of the error."
        },
        {
            "id": 96,
            "type": "multiple_choice",
            "category": "regularization_parameters",
            "difficulty": "intermediate",
            "question": "To *reduce* the regularization impact in a model (to fight underfitting) using Ridge regression, what should be done to the parameter **alpha**?",
            "options": [
                "Increase alpha",
                "Decrease alpha",
                "Set alpha to one",
                "Set alpha to zero"
            ],
            "correct_answer": 1,
            "explanation": "To reduce the regularization, the alpha for Ridge regression should be decreased."
        },
        {
            "id": 97,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Root Mean Squared Logarithmic Error (RMSLE) is resistant to what when both minor and large errors are considered?",
            "options": [
                "Overfitting",
                "Underfitting",
                "Scale dependence",
                "Outliers"
            ],
            "correct_answer": 3,
            "explanation": "RMSLE is resistant to outliers when both minor and large errors are considered."
        },
        {
            "id": 98,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Both MSLE and RMSLE are scale **independent** because the difference of two log values is mathematically the same as the log of what?",
            "options": [
                "The sum of the values",
                "The ratio of the values",
                "The product of the values",
                "The squared difference"
            ],
            "correct_answer": 1,
            "explanation": "The loss function is scale independent as it is a difference of two log values which is the same as log of the ratio of the values."
        },
        {
            "id": 99,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "What is the process of generating more data, often by modifying existing images (e.g., rotating, cropping), in the context of computer vision to combat overfitting?",
            "options": [
                "Feature Selection",
                "Cross-validation",
                "Data Cleaning",
                "Data Augmentation"
            ],
            "correct_answer": 3,
            "explanation": "In the context of computer vision, getting more data can also mean data augmentation."
        },
        {
            "id": 100,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "Which formula correctly represents the calculation for Mean Absolute Percentage Error (MAPE)?",
            "options": [
                "The mean of the squared difference between actual and predicted values.",
                "The mean of the absolute difference, divided by the actual value, expressed as a percentage.",
                "The square root of the mean squared difference.",
                "The mean of the absolute difference between actual and predicted values."
            ],
            "correct_answer": 1,
            "explanation": "MAPE is the mean absolute percentage error, where the absolute difference is divided by the actual value to get a percentage of the error."
        },
        {
            "id": 101,
            "type": "multiple_choice",
            "category": "feature_engineering",
            "difficulty": "intermediate",
            "question": "The transformation $\\log(x)$ is suggested for data with what kind of distribution?",
            "options": [
                "Normal distribution",
                "Heavy left tail",
                "Not-normal distribution",
                "Heavy right tail"
            ],
            "correct_answer": 2,
            "explanation": "$\\log(x)$ is a suggested transformation for data with not-normal distribution."
        },
        {
            "id": 102,
            "type": "multiple_choice",
            "category": "feature_engineering",
            "difficulty": "intermediate",
            "question": "The transformation $\\ln(|x|+1)$ is suggested for data that exhibits what kind of feature?",
            "options": [
                "Normal distribution",
                "Light left tail",
                "Heavy right tail",
                "Not-normal distribution"
            ],
            "correct_answer": 2,
            "explanation": "$\\ln(|x|+1)$ is a suggested transformation for data with heavy right tail."
        },
        {
            "id": 103,
            "type": "multiple_choice",
            "category": "model_complexity_adjustment",
            "difficulty": "intermediate",
            "question": "In the context of Neural Networks, what is a way to *complicate* the model (for underfitting)?",
            "options": [
                "Increase L2 Regularization",
                "Decrease the number of layers",
                "Increase the number of neurons in each layer",
                "Use early stopping"
            ],
            "correct_answer": 2,
            "explanation": "In the context of neural networks, complicating the model means adding more layers/more neurons in each layer / more connections between layers / more filters for CNN, and so on."
        },
        {
            "id": 104,
            "type": "multiple_choice",
            "category": "model_complexity_adjustment",
            "difficulty": "intermediate",
            "question": "To simplify a model (for overfitting) in the context of Neural Networks, what action is generally recommended?",
            "options": [
                "Add more layers",
                "Use more neurons per layer",
                "Use fewer layers/neurons",
                "Decrease the impact of regularization"
            ],
            "correct_answer": 2,
            "explanation": "To simplify the model, reduce the number of degrees of freedom. Fewer layers, fewer neurons, and so on."
        },
        {
            "id": 105,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "Normalized Root Mean Squared Error (NRMSE) is calculated by taking the RMSE and dividing it by what value?",
            "options": [
                "The maximum actual value",
                "The average of the observation values (o)",
                "The mean of the predicted values",
                "The total number of samples (N)"
            ],
            "correct_answer": 1,
            "explanation": "NRMSE divides the RMSE value by $o$ (the average of the observation values)."
        },
        {
            "id": 106,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "intermediate",
            "question": "To *reduce* the regularization impact in a model (to fight underfitting), what should be done to the regularization terms' influence?",
            "options": [
                "Increase the influence of regularization terms.",
                "Keep the influence of regularization terms the same.",
                "Reduce the influence of regularization terms or abandon it altogether.",
                "Introduce new regularization terms."
            ],
            "correct_answer": 2,
            "explanation": "In the case when the model needs to be complicated (underfitting), you should reduce the influence of regularization terms or abandon the regularization at all."
        },
        {
            "id": 107,
            "type": "multiple_choice",
            "category": "model_complexity_adjustment",
            "difficulty": "intermediate",
            "question": "When using a Support Vector Machine (SVM), what adjustment to the `C` parameter is generally required to simplify the model (i.e., increase regularization)?",
            "options": [
                "Increase C",
                "Decrease C",
                "Set C to zero",
                "Keep C constant"
            ],
            "correct_answer": 1,
            "explanation": "To reduce regularization (complicate the model), C for SVM should be increased. Therefore, to increase regularization (simplify the model), C should be decreased."
        },
        {
            "id": 108,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Root Mean Squared Logarithmic Error (RMSLE) is particularly useful when the target variable has a wide range of values because it is what kind of error metric?",
            "options": [
                "A scale-dependent metric that emphasizes large errors.",
                "A scale-independent metric that measures relative error.",
                "A linear error metric that ignores large errors.",
                "A purely quadratic metric, similar to MSE."
            ],
            "correct_answer": 1,
            "explanation": "RMSLE is scale independent as it is the log of the ratio of values, meaning it measures the relative error."
        },
        {
            "id": 109,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Relative Squared Error (RSE) normalizes the total squared error by comparing it against what fundamental baseline model?",
            "options": [
                "A model that always predicts zero.",
                "A simple predictor that uses the mean of the actual values.",
                "The model with the highest possible prediction.",
                "The mean absolute error model."
            ],
            "correct_answer": 1,
            "explanation": "RSE is relative to what it would have been if a simple predictor (the average of the actual values) had been used."
        },
        {
            "id": 110,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "intermediate",
            "question": "What is the common application mentioned for Gaussian Mixture Models?",
            "options": [
                "Classification",
                "Clustering and Density Estimation",
                "Feature Selection",
                "Regression"
            ],
            "correct_answer": 1,
            "explanation": "Gaussian Mixture Models (GMM) are commonly used for clustering, as they assume data is generated from a mixture of several Gaussian distributions."
        },
        {
            "id": 111,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "The core principle to keep in mind when fighting underfitting and overfitting is that to fix them, you need to apply what kind of actions?",
            "options": [
                "The same actions in a different order",
                "Only feature engineering actions",
                "Diametrically opposite actions",
                "Only regularization changes"
            ],
            "correct_answer": 2,
            "explanation": "You may notice that to eliminate underfitting or overfitting, you need to apply diametrically opposite actions."
        },
        {
            "id": 112,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "The Log Cosh Loss is very close to the Huber loss in behavior, exhibiting which characteristic for very large error values?",
            "options": [
                "Quadratic behavior",
                "Cubic behavior",
                "Constant behavior",
                "Linear behavior"
            ],
            "correct_answer": 3,
            "explanation": "Log cosh... has linear behaviour for very large values of error and quadratic for small loss values."
        },
        {
            "id": 113,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "What is the primary practical benefit of using Log Cosh Loss over Huber Loss?",
            "options": [
                "It is scale dependent.",
                "It is simpler to compute.",
                "You do not have to decide the value of the threshold.",
                "It is less sensitive to outliers."
            ],
            "correct_answer": 2,
            "explanation": "The benefit here is you dont have to decide the value of the threshold as in the case of the Huber loss."
        },
        {
            "id": 114,
            "type": "multiple_choice",
            "category": "random_forest_tradeoffs",
            "difficulty": "intermediate",
            "question": "To speed up the Random Forest process, what is a dramatically effective measure related to the features?",
            "options": [
                "Increasing the set of features",
                "Using deeper trees",
                "Reducing the set of features",
                "Using only one decision tree"
            ],
            "correct_answer": 2,
            "explanation": "Reducing the set of features can dramatically speed up the process."
        },
        {
            "id": 115,
            "type": "multiple_choice",
            "category": "supervised_learning_examples",
            "difficulty": "intermediate",
            "question": "An algorithm trained to classify images of cats and dogs, each labeled, is an example of which type of supervised learning?",
            "options": [
                "Regression",
                "Clustering",
                "Classification",
                "Dimensionality Reduction"
            ],
            "correct_answer": 2,
            "explanation": "The training is to classify, and classification is one of the two main types of supervised learning."
        },
        {
            "id": 116,
            "type": "multiple_choice",
            "category": "unsupervised_learning_examples",
            "difficulty": "intermediate",
            "question": "Recommending movies to users based on their ratings of other movies is a task that can be handled by which type of learning algorithm?",
            "options": [
                "Supervised Learning",
                "Unsupervised Learning (e.g., Association/Clustering)",
                "Reinforcement Learning",
                "Transfer Learning"
            ],
            "correct_answer": 1,
            "explanation": "Unsupervised learning algorithms can also be used to train recommendation systems."
        },
        {
            "id": 117,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "The purpose of normalization in loss functions like Relative Absolute Error (RAE) is to compare the model's error against the error produced by what simple predictive technique?",
            "options": [
                "A model that perfectly predicts zero.",
                "A model that predicts the mean of the actual values.",
                "A model with perfect classification accuracy.",
                "A model using only a linear function."
            ],
            "correct_answer": 1,
            "explanation": "RAE involves dividing the total absolute error by the absolute difference between the mean and the actual value, comparing it against the mean as a baseline predictor."
        },
        {
            "id": 118,
            "type": "multiple_choice",
            "category": "random_forest_problems",
            "difficulty": "intermediate",
            "question": "What distinguishes the interpretation process of a Random Forest from that of a single Decision Tree?",
            "options": [
                "RFs are generally easier to interpret.",
                "RFs have a steeper learning curve for interpretation.",
                "RFs interpretation follows a single path.",
                "RFs do not require any interpretation."
            ],
            "correct_answer": 1,
            "explanation": "A random forest is a tad more complicated to interpret... the learning curve is steep."
        },
        {
            "id": 119,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "Why does feature engineering and feature selection make almost no sense in the context of deep neural networks?",
            "options": [
                "The network is too simple to use them.",
                "The network finds dependencies in the data itself.",
                "They cause underfitting.",
                "They are too slow for deep learning."
            ],
            "correct_answer": 1,
            "explanation": "In the context of neural networks, feature engineering and feature selection make almost no sense because the network finds dependencies in the data itself."
        },
        {
            "id": 120,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Which loss function is defined as the square root of the average of the squared difference between the actual and predicted values?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Root Mean Squared Error (RMSE)",
                "Mean Bias Error (MBE)"
            ],
            "correct_answer": 2,
            "explanation": "Root Mean Squared Error (RMSE) is the square root of MSE."
        },
        {
            "id": 121,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "In the Quantile Loss function, what does the parameter $\\gamma$ (gamma) represent?",
            "options": [
                "The total number of samples.",
                "A threshold for error magnitude.",
                "The predicted value.",
                "The required quantile."
            ],
            "correct_answer": 3,
            "explanation": "The quantile values are chosen based on how we wish to balance the positive and the negative errors, and $\\gamma$ represents the required quantile."
        },
        {
            "id": 122,
            "type": "multiple_choice",
            "category": "model_complexity_adjustment",
            "difficulty": "intermediate",
            "question": "When addressing underfitting, you should consider moving from a simpler model to a more what kind of model, one a priori capable of restoring more complex dependencies?",
            "options": [
                "Linear model",
                "Powerful model",
                "Regularized model",
                "Unsupervised model"
            ],
            "correct_answer": 1,
            "explanation": "To complicate the model... Sometimes this means directly trying a more powerful model - one that is a priori capable of restoring more complex dependencies."
        },
        {
            "id": 123,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Which loss function is simply defined as the average of the difference between the actual and predicted values, with no absolute or square terms?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Mean Bias Error (MBE)",
                "Root Mean Squared Error (RMSE)"
            ],
            "correct_answer": 2,
            "explanation": "Mean Bias Error (MBE) is defined as the average of the raw difference between the actual and predicted values."
        },
        {
            "id": 124,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "intermediate",
            "question": "What characteristic of a model is often improved when strong regularization is applied, making it perform better than an initially simple model?",
            "options": [
                "Its interpretability",
                "Its simplicity (even when complex)",
                "Its training speed",
                "Its bias (High bias)"
            ],
            "correct_answer": 1,
            "explanation": "Complex models with strong regularization often perform better than initially simple models."
        },
        {
            "id": 125,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Which loss function calculates the mean of the squared difference between the log of (actual value + 1) and the log of (predicted value + 1)?",
            "options": [
                "Root Mean Squared Logarithmic Error (RMSLE)",
                "Mean Squared Error (MSE)",
                "Mean Squared Logarithmic Error (MSLE)",
                "Log Cosh Loss"
            ],
            "correct_answer": 2,
            "explanation": "Mean Squared Logarithmic Error (MSLE) measures the mean squared difference between the log-transformed values."
        },
        {
            "id": 126,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "What is the key difference between Mean Squared Logarithmic Error (MSLE) and Root Mean Squared Logarithmic Error (RMSLE)?",
            "options": [
                "MSLE is for classification, RMSLE is for regression.",
                "RMSLE takes the square root of the MSLE calculation.",
                "RMSLE is scale-dependent, MSLE is not.",
                "MSLE ignores underestimates, RMSLE penalizes them."
            ],
            "correct_answer": 1,
            "explanation": "RMSLE is the root mean square version of MSLE (i.e., it is the square root of MSLE)."
        },
        {
            "id": 127,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Normalized Root Mean Squared Error (NRMSE) eases comparison between models of different scales or datasets by overcoming what dependency?",
            "options": [
                "Feature dependency",
                "Log dependency",
                "Scale dependency",
                "Outlier dependency"
            ],
            "correct_answer": 2,
            "explanation": "NRMSE overcomes the scale dependency and eases comparison between models of different scales or datasets."
        },
        {
            "id": 128,
            "type": "multiple_choice",
            "category": "feature_selection",
            "difficulty": "intermediate",
            "question": "Linear models often work worse if some features are highly correlated, meaning they are what?",
            "options": [
                "Independent",
                "Randomly distributed",
                "Dependent",
                "Normally distributed"
            ],
            "correct_answer": 2,
            "explanation": "Linear models often work worse if some features are dependent - highly correlated."
        },
        {
            "id": 129,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "If a model is suffering from a high **bias** problem (underfitting), getting more data will most likely result in what outcome?",
            "options": [
                "Fix the problem immediately",
                "Help significantly",
                "Not help",
                "Cause high variance"
            ],
            "correct_answer": 2,
            "explanation": "Underfitting is high bias, and getting more data will not help in the case of underfitting."
        },
        {
            "id": 130,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "intermediate",
            "question": "If the training error is large, and the validation/test error is large, what is the 'diagnosis' for the model?",
            "options": [
                "Overfitting",
                "Good model",
                "Underfitting",
                "Train and test sets are different"
            ],
            "correct_answer": 2,
            "explanation": "Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large and val/test error is large too."
        },
        {
            "id": 131,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_detection",
            "difficulty": "intermediate",
            "question": "If the training error is very small, and the validation/test error is large, what is the 'diagnosis' for the model?",
            "options": [
                "Underfitting",
                "Good model",
                "Overfitting",
                "Train and test sets are different"
            ],
            "correct_answer": 2,
            "explanation": "Overfitting means that your model makes not accurate predictions. In this case, train error is very small and val/test error is large."
        },
        {
            "id": 132,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "intermediate",
            "question": "Which regularization technique involves randomly setting a fraction of neurons to zero during training in a neural network?",
            "options": [
                "L1 Regularization",
                "L2 Regularization",
                "Early Stopping",
                "Dropout"
            ],
            "correct_answer": 3,
            "explanation": "Dropout is listed as a main regularization method in the context of neural networks."
        },
        {
            "id": 133,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "intermediate",
            "question": "What is the primary role of the regularization term in the loss function?",
            "options": [
                "Increase parameter values",
                "Discourage simplicity in the model",
                "Require the model to keep parameter values as small as possible",
                "Increase the training error"
            ],
            "correct_answer": 2,
            "explanation": "The regularization term requires the model to keep parameter values as small as possible, so requires the model to be as simple as possible."
        },
        {
            "id": 134,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "intermediate",
            "question": "The Huber Loss is defined as quadratic for small errors and what for errors beyond a certain threshold?",
            "options": [
                "Cubic",
                "Logarithmic",
                "Linear",
                "Zero"
            ],
            "correct_answer": 2,
            "explanation": "Huber loss is a combination of two loss functions, quadratic and linear. The loss is linear for loss values beyond the threshold."
        },
        {
            "id": 135,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "intermediate",
            "question": "Random Forests use which method to combine a large number of trees at the end of the process?",
            "options": [
                "Weighted averaging",
                "Majority rules or averages",
                "Sequential fitting",
                "Residual calculation"
            ],
            "correct_answer": 1,
            "explanation": "Random forests are a large number of trees, combined (using averages or 'majority rules') at the end of the process."
        },
        {
            "id": 136,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "The combination of high bias and high variance is described as what kind of situation in the documents?",
            "options": [
                "A good result",
                "The optimal solution",
                "Very bad algorithm (unlikely to be seen)",
                "The best trade-off"
            ],
            "correct_answer": 2,
            "explanation": "High bias, high variance - very bad algorithm. You will most likely never see this."
        },
        {
            "id": 137,
            "type": "multiple_choice",
            "category": "unsupervised_learning_characteristics",
            "difficulty": "intermediate",
            "question": "What is a **pro** of using Unsupervised Learning that Supervised Learning lacks?",
            "options": [
                "It is less versatile.",
                "It is always more accurate for similar tasks.",
                "It can work with unlabeled data.",
                "It is generally simpler."
            ],
            "correct_answer": 2,
            "explanation": "A pro is that Unsupervised Learning can work with unlabeled data, making it more versatile."
        },
        {
            "id": 138,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "In Quantile Loss for a required quantile $\\gamma$, the weight $(\\gamma - 1)$ is applied to which type of prediction error?",
            "options": [
                "Errors where the predicted value is lower than the actual value (overestimation).",
                "Errors where the predicted value is higher than the actual value (underestimation).",
                "All positive errors.",
                "All negative errors."
            ],
            "correct_answer": 1,
            "explanation": "The term with $(\\gamma-1)$ is applied when $y_i < \\hat{y}_i$. If $\\gamma$ is a probability (0-1), then $(\\gamma-1)$ is negative, applying to the absolute error. This term typically punishes underestimation (predicted value is lower than actual value)."
        },
        {
            "id": 139,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "In Quantile Loss for a required quantile $\\gamma$, the weight $\\gamma$ is applied to which type of prediction error?",
            "options": [
                "Errors where the predicted value is lower than the actual value (overestimation).",
                "Errors where the predicted value is higher than the actual value (underestimation).",
                "All positive errors.",
                "All negative errors."
            ],
            "correct_answer": 0,
            "explanation": "The term with $(\\gamma)$ is applied when $y_i \\ge \\hat{y}_i$. If $\\gamma$ is a probability (0-1), this term typically punishes overestimation (predicted value is higher than actual value)."
        },
        {
            "id": 140,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "intermediate",
            "question": "Gradient Boosting Machines (GBM) use what type of model that works in a forward stage-wise manner to introduce weak learners?",
            "options": [
                "Parallel model",
                "Linear model",
                "Additive model (ensemble)",
                "Independent model"
            ],
            "correct_answer": 2,
            "explanation": "This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners."
        },
        {
            "id": 141,
            "type": "multiple_choice",
            "category": "supervised_learning_characteristics",
            "difficulty": "intermediate",
            "question": "Supervised learning algorithms are primarily noted for their high accuracy and reliability when what is abundant?",
            "options": [
                "Unlabeled data",
                "Continuous data",
                "Noise",
                "Labeled data"
            ],
            "correct_answer": 3,
            "explanation": "A pro is high accuracy and reliability when labeled data is abundant."
        },
        {
            "id": 142,
            "type": "multiple_choice",
            "category": "supervised_learning_characteristics",
            "difficulty": "intermediate",
            "question": "A major **con** of supervised learning is the risk that it may not handle unseen data well if it is not what?",
            "options": [
                "Labeled",
                "Generalized",
                "Cleaned",
                "Regularized"
            ],
            "correct_answer": 1,
            "explanation": "A con is that supervised learning may not handle unseen data well if not properly generalized."
        },
        {
            "id": 143,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "What is the primary way to adjust model complexity that comes to mind based on the simple intuition to simplify or complicate the model?",
            "options": [
                "Applying cross-validation",
                "Trying a more simple or more complex algorithm (model)",
                "Changing the number of data points",
                "Applying data cleaning"
            ],
            "correct_answer": 1,
            "explanation": "The easiest way that comes to mind based on the intuition above is to try a more simple or more complex algorithm (model)."
        },
        {
            "id": 144,
            "type": "multiple_choice",
            "category": "overfitting_tradeoffs",
            "difficulty": "intermediate",
            "question": "What happens when an algorithm has **low bias** but **high variance**?",
            "options": [
                "The algorithm outputs similar but wrong predictions.",
                "The algorithm outputs very different predictions for similar data.",
                "The model is a good fit.",
                "The model is too simple for the data."
            ],
            "correct_answer": 1,
            "explanation": "Low bias, high variance - overfitting - the algorithm outputs very different predictions for similar data."
        },
        {
            "id": 145,
            "type": "multiple_choice",
            "category": "underfitting_tradeoffs",
            "difficulty": "intermediate",
            "question": "What happens when an algorithm has **high bias** but **low variance**?",
            "options": [
                "The algorithm outputs very different predictions for similar data.",
                "The algorithm outputs similar predictions for similar data, but predictions are wrong.",
                "The model is a good fit.",
                "The model is too complex for the data."
            ],
            "correct_answer": 1,
            "explanation": "High bias, low variance - underfitting - the algorithm outputs similar predictions for similar data, but predictions are wrong (algorithm 'miss')."
        },
        {
            "id": 146,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "intermediate",
            "question": "A decision tree is considered fraught with problems because a tree generated from 99 data points might differ **significantly** from a tree generated with how many different data points?",
            "options": [
                "100 different data points",
                "10 different data points",
                "Just one different data point",
                "1000 different data points"
            ],
            "correct_answer": 2,
            "explanation": "A tree generated from 99 data points might differ significantly from a tree generated with just one different data point."
        },
        {
            "id": 147,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "intermediate",
            "question": "What are the names for the regularization methods L1 and L2 when applied to linear regression?",
            "options": [
                "Huber Loss and Log Cosh Loss",
                "Dropout and Early Stopping",
                "Ridge and Lasso regression",
                "PCA and t-SNE"
            ],
            "correct_answer": 2,
            "explanation": "L1/L2 coefficients for linear regression correspond to Lasso (L1) and Ridge (L2) regression."
        },
        {
            "id": 148,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "intermediate",
            "question": "What is a major advantage of unsupervised learning that makes it more versatile?",
            "options": [
                "High accuracy and reliability",
                "Works with labeled data",
                "Can work with unlabeled data",
                "Generally simpler complexity"
            ],
            "correct_answer": 2,
            "explanation": "A pro is that Unsupervised Learning can work with unlabeled data, making it more versatile."
        },
        {
            "id": 149,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "intermediate",
            "question": "Root Mean Squared Error (RMSE) has the same units as the output variable, which is an advantage over which loss function with different (squared) units?",
            "options": [
                "Mean Absolute Error (MAE)",
                "Mean Squared Error (MSE)",
                "Mean Bias Error (MBE)",
                "Mean Absolute Percentage Error (MAPE)"
            ],
            "correct_answer": 1,
            "explanation": "RMSE takes the square root of MSE, putting the error back into the same units as the output variable (which is an advantage for interpretability over MSE's squared units)."
        },
        {
            "id": 150,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "intermediate",
            "question": "Techniques like data cleaning and cross-validation are considered what in any machine learning project, even though they can help combat overfitting?",
            "options": [
                "Optional practices",
                "Advanced tools",
                "Common practices",
                "Underfitting techniques"
            ],
            "correct_answer": 2,
            "explanation": "Data cleaning and cross-validation or hold-out validation are considered common practices in any machine learning project."
        },
        {
            "id": 151,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "A polynomial can perfectly match $N$ distinct data points. What degree of polynomial is required to perfectly match a dataset of 140 training data points?",
            "options": [
                "13",
                "14",
                "139",
                "140"
            ],
            "correct_answer": 2,
            "explanation": "To perfectly match $N$ data points, you need a polynomial of degree $N-1$. For 140 data points, a 139-degree polynomial is required."
        },
        {
            "id": 152,
            "type": "multiple_choice",
            "category": "random_forest_advantages",
            "difficulty": "advanced",
            "question": "Why are Random Forests generally considered to have lower variance than a single, deep decision tree?",
            "options": [
                "They use an additive model.",
                "They combine solutions by averaging across a large number of trees built on different samples/features.",
                "They are explicitly designed to introduce weak learners sequentially.",
                "They are less complicated to interpret."
            ],
            "correct_answer": 1,
            "explanation": "Random Forests reduce variance by using different samples, random feature subsets, and combining small trees, essentially averaging out solutions from a large number of trees."
        },
        {
            "id": 153,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "In the context of ensemble methods, Gradient Boosting's sequential process is fundamentally aimed at reducing what kind of error?",
            "options": [
                "Variance",
                "Irreducible Error",
                "Bias",
                "Overfitting"
            ],
            "correct_answer": 2,
            "explanation": "Gradient Boosting builds one tree at a time (additive model) to improve the shortcomings of existing weak learners, which primarily focuses on reducing the systematic error (bias) of the ensemble model."
        },
        {
            "id": 154,
            "type": "multiple_choice",
            "category": "loss_function_evaluation",
            "difficulty": "advanced",
            "question": "Why is Mean Bias Error (MBE) generally a poor standalone metric for model performance, despite identifying bias direction?",
            "options": [
                "It is non-differentiable at zero.",
                "It penalizes underestimates more than overestimates.",
                "Errors can cancel out, misleadingly resulting in a zero loss.",
                "It is highly sensitive to outliers."
            ],
            "correct_answer": 2,
            "explanation": "The issue with this loss (MBE) is that some times the errors may cancel out between individual samples resulting in a zero loss which may be misleading."
        },
        {
            "id": 155,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "advanced",
            "question": "For a very complex model that is overfitting, why might increasing the dataset size *not* help to reduce the overfitting, according to the document's logic?",
            "options": [
                "Because the number of features is too small.",
                "Because the model is so complex that even the larger dataset size is not enough to constrain it.",
                "Because the model is too simple (underfitting).",
                "Because the model is already using regularization."
            ],
            "correct_answer": 1,
            "explanation": "If we had initially trained a VERY complex model (for example, a 150-degree polynomial), such an increase in data would not have helped."
        },
        {
            "id": 156,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "advanced",
            "question": "Which loss function is defined piecewise, switching from a quadratic form for small errors to a linear form for large errors based on a threshold $\\delta$?",
            "options": [
                "Mean Squared Error (MSE)",
                "Mean Absolute Error (MAE)",
                "Log Cosh Loss",
                "Huber Loss"
            ],
            "correct_answer": 3,
            "explanation": "Huber loss is a combination of two loss functions (quadratic and linear), where the behavior is defined by the value of the threshold."
        },
        {
            "id": 157,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_applications",
            "difficulty": "advanced",
            "question": "A task involves grouping customers for targeted marketing, and the data has no pre-existing 'group' labels. Which ML approach is most appropriate?",
            "options": [
                "Supervised Learning (Classification)",
                "Unsupervised Learning (Clustering)",
                "Supervised Learning (Regression)",
                "Reinforcement Learning"
            ],
            "correct_answer": 1,
            "explanation": "Customer Segmentation is an example of Unsupervised Learning, which finds patterns in unlabeled data."
        },
        {
            "id": 158,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_principles",
            "difficulty": "advanced",
            "question": "How does a model exhibit 'high variance' (overfitting) in its predictions?",
            "options": [
                "It outputs similar, but wrong, predictions for similar data.",
                "It makes accurate predictions but the initial assumption is incorrect.",
                "It outputs very different predictions for similar data.",
                "It has a large train error and a large test error."
            ],
            "correct_answer": 2,
            "explanation": "Overfitting (high variance) means that your algorithm can't make accurate predictions - changing the input data only a little, the model output changes very much."
        },
        {
            "id": 159,
            "type": "multiple_choice",
            "category": "overfitting_underfitting_principles",
            "difficulty": "advanced",
            "question": "How does a model exhibit 'high bias' (underfitting) in its predictions?",
            "options": [
                "It outputs very different predictions for similar data.",
                "It makes accurate predictions, but the initial assumption is incorrect.",
                "It has a very small train error and a large test error.",
                "It is too complex for the data."
            ],
            "correct_answer": 1,
            "explanation": "Underfitting (high bias) means the algorithm makes accurate predictions, but the initial assumption about the data is incorrect (too simple)."
        },
        {
            "id": 160,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "Root Mean Squared Logarithmic Error (RMSLE) is often preferred for skewed target variables because it penalizes which type of errors more heavily?",
            "options": [
                "Outliers",
                "Overestimates",
                "Underestimates",
                "Errors close to zero"
            ],
            "correct_answer": 2,
            "explanation": "Due to the loss being log it penalizes underestimates more than overestimates."
        },
        {
            "id": 161,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "If a large number of independent decision trees are used, why does the error of the aggregated result (Random Forest) converge very close to the true answer?",
            "options": [
                "Because each tree is a strong predictor.",
                "Because the weak learners correct the residuals sequentially.",
                "Because averaging out a large number of independent tree solutions reduces the high variance.",
                "Because the process starts combining at the beginning."
            ],
            "correct_answer": 2,
            "explanation": "If there was a way to generate a very large number of trees, averaging out their solutions, then you'll likely get an answer that is going to be very close to the true answer."
        },
        {
            "id": 162,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "The intuition 'to fix underfitting, you should complicate the model' is explicitly implemented by using what technique for a neural network?",
            "options": [
                "L1 Regularization",
                "Early Stopping",
                "Adding more layers/neurons",
                "Removing features"
            ],
            "correct_answer": 2,
            "explanation": "In the context of neural networks, complicating the model means adding more layers/more neurons in each layer / more connections between layers / more filters for CNN, and so on."
        },
        {
            "id": 163,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What is the consequence of 'misdiagnosing' a model (e.g., thinking a model is underfitting when it is overfitting) in terms of wasted effort?",
            "options": [
                "Only minor loss in accuracy.",
                "You can spend a lot of time and money on empty work, applying diametrically opposite actions.",
                "The model will immediately crash.",
                "You will be forced to use an unsupervised model."
            ],
            "correct_answer": 1,
            "explanation": "If you initially 'misdiagnosed' your model, you can spend a lot of time and money on empty work, applying diametrically opposite actions."
        },
        {
            "id": 164,
            "type": "multiple_choice",
            "category": "regularization_techniques",
            "difficulty": "advanced",
            "question": "Why is regularization considered a 'very powerful tool' despite its indirect and forced simplification of the model?",
            "options": [
                "It always leads to a smaller training error.",
                "Complex models with strong regularization often perform better than initially simple models.",
                "It eliminates the need for feature engineering.",
                "It only works with linear models."
            ],
            "correct_answer": 1,
            "explanation": "Complex models with strong regularization often perform better than initially simple models, so this is a very powerful tool."
        },
        {
            "id": 165,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "advanced",
            "question": "Which loss function is an RMSE variant without dimensions, where the RMSE value is scaled against the predicted values?",
            "options": [
                "Normalized Root Mean Squared Error (NRMSE)",
                "Relative Root Mean Squared Error (RRMSE)",
                "Mean Absolute Percentage Error (MAPE)",
                "Mean Squared Logarithmic Error (MSLE)"
            ],
            "correct_answer": 1,
            "explanation": "Relative Root Mean Squared Error (RRMSE) is an RMSE variant without dimensions, scaled against the actual value and normalized by the root mean square value."
        },
        {
            "id": 166,
            "type": "multiple_choice",
            "category": "feature_selection",
            "difficulty": "advanced",
            "question": "If a linear model works worse due to highly correlated features, which technique should be used to improve performance by selecting the most useful information?",
            "options": [
                "Data Augmentation",
                "Feature Engineering (creating new features)",
                "Feature Selection",
                "Increasing L2 Regularization"
            ],
            "correct_answer": 2,
            "explanation": "To simplify the model, feature selection approaches should be used to select only those features that carry the maximum amount of useful information."
        },
        {
            "id": 167,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_applications",
            "difficulty": "advanced",
            "question": "Which real-world application is cited as a use case for supervised learning, focusing on predicting a specific outcome?",
            "options": [
                "Google News (Pattern Discovery)",
                "Customer Segmentation (Clustering)",
                "Spam filters (Classification/Prediction)",
                "Exploring hidden data patterns (Exploratory Analysis)"
            ],
            "correct_answer": 2,
            "explanation": "Spam filters are an example of Supervised Learning, which is used for predicting a specific outcome."
        },
        {
            "id": 168,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "The Log Cosh Loss is smooth and convex, which is an advantage over the Mean Absolute Error (MAE) due to MAE's main problem in optimization, which is what?",
            "options": [
                "Its scale independence",
                "Its resistance to outliers",
                "Its non-differentiability at zero",
                "Its tendency to penalize overestimates more"
            ],
            "correct_answer": 2,
            "explanation": "MAE is not differentiable at zero, which can complicate gradient-based optimization. Log Cosh Loss is smooth (differentiable)."
        },
        {
            "id": 169,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your model is underfitting (high bias), and you are limited in your ability to generate more complex features, what *other* technique mentioned is most appropriate to combat the issue?",
            "options": [
                "Increase regularization impact (simplify model)",
                "Increase the number of data points (won't help high bias)",
                "Try a more powerful model with a larger number of parameters",
                "Use a smaller quantity of features (simplify model)"
            ],
            "correct_answer": 2,
            "explanation": "To fix underfitting, you should complicate the model, by trying a more powerful model with a larger number of parameters (e.g., Ensemble learning, or more layers in a NN)."
        },
        {
            "id": 170,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "advanced",
            "question": "What is the risk of using an Unsupervised Learning algorithm for a task that could be solved by Supervised Learning?",
            "options": [
                "It will be more accurate for similar tasks.",
                "It will be generally simpler.",
                "It may yield less accurate results compared to the supervised model.",
                "It will require labeled data."
            ],
            "correct_answer": 2,
            "explanation": "A con of Unsupervised Learning is that it is less accurate compared to supervised learning for similar tasks."
        },
        {
            "id": 171,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "A model uses a 13-degree polynomial on a cubic dataset, resulting in Training MSE = 0.00 and Testing MSE = 127356.21. This indicates what problem?",
            "options": [
                "Underfitting (High Bias)",
                "Overfitting (High Variance)",
                "Good Model (Low Bias, Low Variance)",
                "Underfitting (Low Bias)"
            ],
            "correct_answer": 1,
            "explanation": "Training MSE is very small (0.00) and Testing MSE is very large (127356.21), which is the classic sign of Overfitting (High Variance)."
        },
        {
            "id": 172,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "A model uses a linear function on a cubic dataset, resulting in Training MSE = 119.42 and Testing MSE = 88.15. This indicates what problem?",
            "options": [
                "Overfitting (Low Bias)",
                "Underfitting (High Bias)",
                "Good Model (Low Bias, Low Variance)",
                "Overfitting (High Variance)"
            ],
            "correct_answer": 1,
            "explanation": "Both Training MSE and Testing MSE are large, and the model is too simple (linear on cubic data), which is the classic sign of Underfitting (High Bias)."
        },
        {
            "id": 173,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What is the technique to increase the number of features that is achieved by using polynomial terms like $X, X^2, X^3$?",
            "options": [
                "Feature Selection",
                "Data Augmentation",
                "Feature Transformation",
                "Polynomial Features (Feature Engineering)"
            ],
            "correct_answer": 3,
            "explanation": "Polynomial features are a type of artificial feature engineering."
        },
        {
            "id": 174,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "The concept of an optimization problem in machine learning is always aimed at performing what action on the loss function?",
            "options": [
                "Maximize it",
                "Keep it constant",
                "Minimize it",
                "Set it to infinity"
            ],
            "correct_answer": 2,
            "explanation": "An optimization problem seeks to minimize a loss function."
        },
        {
            "id": 175,
            "type": "multiple_choice",
            "category": "decision_tree_problems",
            "difficulty": "advanced",
            "question": "What is the formal term for the disadvantage of a decision tree where the initial assumption about the data is wrong and too simple (e.g., linear relationship when quadratic)?",
            "options": [
                "Overfitting",
                "Variance error",
                "Bias error",
                "High variance"
            ],
            "correct_answer": 2,
            "explanation": "Bias is high when the hypothesis about data distribution is wrong and too simple (underfitting)."
        },
        {
            "id": 176,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "Which parameter for a Support Vector Machine (SVM) should be **increased** if the model is **underfitting** (high bias)?",
            "options": [
                "Gamma",
                "C",
                "Alpha",
                "Maximum tree depth"
            ],
            "correct_answer": 1,
            "explanation": "To fight underfitting, the model should be complicated (less regularization). To reduce the regularization, C for SVM should be increased."
        },
        {
            "id": 177,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "Which parameter for a Ridge Regression model should be **decreased** if the model is **underfitting** (high bias)?",
            "options": [
                "Alpha",
                "C",
                "Maximum tree depth",
                "Number of features"
            ],
            "correct_answer": 0,
            "explanation": "To fight underfitting, the model should be complicated (less regularization). To reduce the regularization, the alpha for Ridge regression should be decreased."
        },
        {
            "id": 178,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your model is overfitting (low bias, high variance), what action should you take regarding the maximum tree depth for an ensemble of Decision Trees?",
            "options": [
                "Increase it",
                "Decrease it",
                "Set it to infinity",
                "Keep it constant"
            ],
            "correct_answer": 1,
            "explanation": "To fight overfitting, the model should be simplified (more regularization). Decreasing the maximum tree depth simplifies the tree."
        },
        {
            "id": 179,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "How do Gradient Boosting Machines improve the shortcomings (residuals) of existing weak learners in the ensemble?",
            "options": [
                "By averaging the strong learners' outputs.",
                "By training each new tree on a random subset of features.",
                "By introducing a new weak learner to compensate for the residuals (shortcomings).",
                "By using majority rules at the end of the process."
            ],
            "correct_answer": 2,
            "explanation": "This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners."
        },
        {
            "id": 180,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "The primary mathematical reason for adding 1 to the values when calculating MSLE and RMSLE is to prevent what issue?",
            "options": [
                "Errors from canceling out.",
                "The error from becoming too large.",
                "The logarithm's argument from becoming zero or negative.",
                "The loss function from being scale-independent."
            ],
            "correct_answer": 2,
            "explanation": "Adding 1 to $y_i$ and $\\hat{y}_i$ ensures the term $\\log(y_i + 1)$ avoids a mathematical error if $y_i$ is 0 or less (as $\\log(0)$ is undefined)."
        },
        {
            "id": 181,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "Why is Mean Squared Error (MSE) generally preferred over Mean Absolute Error (MAE) in optimization using gradient descent?",
            "options": [
                "MSE is less sensitive to outliers.",
                "MSE is not differentiable at zero.",
                "MSE's quadratic function ensures a smooth and continuous gradient, even near the minimum.",
                "MSE's range is finite."
            ],
            "correct_answer": 2,
            "explanation": "MSE convergence is smooth as the gradient becomes smaller as the loss decreases. MAE is not differentiable at zero."
        },
        {
            "id": 182,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "Why did a very complex model (degree 13) trained on a much larger dataset (10x larger) show a better testing MSE than on the initial smaller dataset?",
            "options": [
                "The model became simpler.",
                "The increase in data size made the previous complexity no longer 'too complex' for the new training size.",
                "The bias error increased.",
                "The regularization term was implicitly increased."
            ],
            "correct_answer": 1,
            "explanation": "By getting 10 times more data, the training size increased significantly. The 13-degree polynomial was no longer complex enough to perfectly match the data, leading to better generalization and an improved testing MSE."
        },
        {
            "id": 183,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "If a model's Relative Absolute Error (RAE) is 0.05, how is the efficacy of the predictive model assessed?",
            "options": [
                "The model is a bad predictor (value near infinity).",
                "The model is too complex (value near 1).",
                "The model is a good predictor (value near zero).",
                "The model is completely unbiased."
            ],
            "correct_answer": 2,
            "explanation": "RAE has a possible value between 0 and infinity. Values near zero, with zero being the best value, are characteristics of a good model."
        },
        {
            "id": 184,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "advanced",
            "question": "Which clustering algorithm is an iterative, non-overlapping process that assigns data points to the nearest cluster mean?",
            "options": [
                "Hierarchical Clustering",
                "Principal Component Analysis (PCA)",
                "Gaussian Mixture Models",
                "K-means Clustering"
            ],
            "correct_answer": 3,
            "explanation": "K-means Clustering divides data into non-overlapping subsets and uses cluster means (centroids)."
        },
        {
            "id": 185,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your model has low bias and low variance, what is the 'diagnosis' for the model?",
            "options": [
                "Underfitting",
                "Overfitting",
                "Good result",
                "Very bad algorithm"
            ],
            "correct_answer": 2,
            "explanation": "Low bias, low variance - is a good result, just right."
        },
        {
            "id": 186,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "The fundamental goal of an ensemble method, combining weak learners to form a strong learner, is to increase what of the overall model?",
            "options": [
                "Interpretability",
                "Complexity (in all cases)",
                "Training time",
                "Accuracy (or robustness)"
            ],
            "correct_answer": 3,
            "explanation": "Ensemble methods like Random Forest and Gradient Boosting aim to produce a more robust model with better accuracy."
        },
        {
            "id": 187,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "advanced",
            "question": "What is the key difference between Relative Absolute Error (RAE) and Relative Squared Error (RSE) in terms of the error calculation?",
            "options": [
                "RAE uses log values, RSE uses linear values.",
                "RAE uses the absolute difference, RSE uses the squared difference.",
                "RAE is not normalized, RSE is normalized.",
                "RAE uses the predicted value, RSE uses the actual value."
            ],
            "correct_answer": 1,
            "explanation": "RAE is based on the total absolute error, while RSE is based on the total squared error."
        },
        {
            "id": 188,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "advanced",
            "question": "The goal of Dimensionality Reduction algorithms like PCA or t-SNE is to simplify the data by transforming variables to a new set of what?",
            "options": [
                "More complex features",
                "Higher dimensions",
                "Fewer dimensions (components)",
                "Independent variables"
            ],
            "correct_answer": 2,
            "explanation": "Principal Component Analysis (PCA) reduces the dimensionality of the data."
        },
        {
            "id": 189,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your validation error is large, the training error is small, and the test error is large, what is the most likely issue?",
            "options": [
                "Underfitting",
                "Overfitting (High Variance)",
                "Good model",
                "Train and test sets are different"
            ],
            "correct_answer": 1,
            "explanation": "Small training error and large validation/test error is a diagnosis of Overfitting."
        },
        {
            "id": 190,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "When detecting a 'Good model', what should the test error value be in relation to the validation error value?",
            "options": [
                "Much larger",
                "Much smaller",
                "Approximately the same",
                "Zero"
            ],
            "correct_answer": 2,
            "explanation": "The test error and validation error are approximately the same when everything is fine."
        },
        {
            "id": 191,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What is the primary reason why Decision Trees have a high variance error?",
            "options": [
                "They are inherently simple models (high bias).",
                "They have a linear equation restriction.",
                "Tiny changes in the training data have the potential to cause large changes in the final result.",
                "They use an additive model."
            ],
            "correct_answer": 2,
            "explanation": "Decision trees have high variance, which means that tiny changes in the training data have the potential to cause large changes in the final result."
        },
        {
            "id": 192,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What regularization technique is essential for very complex neural networks to prevent learning the noise of the training data and overfitting?",
            "options": [
                "Feature Engineering",
                "Dropout",
                "PCA",
                "Ensemble Learning"
            ],
            "correct_answer": 1,
            "explanation": "Dropout is listed as a main regularization method used for neural networks to fight overfitting."
        },
        {
            "id": 193,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "What is the characteristic of Relative Root Mean Squared Error (RRMSE) that makes it useful for comparing models across different scales?",
            "options": [
                "It is scale dependent.",
                "It is calculated without units (dimensionless).",
                "It focuses only on the mean absolute error.",
                "It penalizes only large errors."
            ],
            "correct_answer": 1,
            "explanation": "RRMSE is an RMSE variant without dimensions."
        },
        {
            "id": 194,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What is the risk if a linear model is trained on a dataset where some features are highly correlated (dependent)?",
            "options": [
                "Underfitting (High Bias)",
                "A perfectly fit model",
                "Worse performance (as correlated features cause problems)",
                "Faster training time"
            ],
            "correct_answer": 2,
            "explanation": "Linear models often work worse if some features are dependent - highly correlated."
        },
        {
            "id": 195,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "advanced",
            "question": "What is the specific goal of a clustering algorithm in an unsupervised learning context?",
            "options": [
                "To predict a continuous outcome variable.",
                "To classify data points into pre-defined labels.",
                "To divide data into non-overlapping subsets or build a tree of clusters.",
                "To predict the creditworthiness of a customer."
            ],
            "correct_answer": 2,
            "explanation": "Clustering algorithms (like K-means and Hierarchical Clustering) are designed to divide data into subsets or build a tree of clusters."
        },
        {
            "id": 196,
            "type": "multiple_choice",
            "category": "regression_loss_formulas",
            "difficulty": "advanced",
            "question": "Which loss function is an ideal combination of quadratic and linear scoring algorithms?",
            "options": [
                "Log Cosh Loss",
                "Mean Absolute Error (MAE)",
                "Huber Loss",
                "Quantile Loss"
            ],
            "correct_answer": 2,
            "explanation": "Huber loss is an ideal combination of quadratic and linear scoring algorithms."
        },
        {
            "id": 197,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What is the primary strategy to simplify an ensemble model, like a Gradient Boosting Machine, to combat overfitting?",
            "options": [
                "Increase the number of models in boosting.",
                "Decrease regularization (e.g., reduce L1/L2).",
                "Use fewer layers/number of neurons per layer (if using neural networks).",
                "Reduce the number of models/trees in boosting."
            ],
            "correct_answer": 3,
            "explanation": "To fight overfitting, the model should be simplified. To complicate the model, you increase the number of models in boosting, so the opposite (reducing the number) simplifies it."
        },
        {
            "id": 198,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "Which technique is considered a common practice in any ML project, but is also explicitly used to evaluate performance on unseen data and is critical for detecting both underfitting and overfitting?",
            "options": [
                "Feature Engineering",
                "Polynomial Features",
                "Cross-validation or hold-out validation",
                "L1/L2 Regularization"
            ],
            "correct_answer": 2,
            "explanation": "Data cleaning and cross-validation or hold-out validation are common practices that can be considered tools to combat overfitting/underfitting."
        },
        {
            "id": 199,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "Log Cosh Loss is a continuous, differentiable function that smooths out the behavior of which other loss function?",
            "options": [
                "Mean Squared Error (MSE)",
                "Mean Absolute Error (MAE)",
                "Huber Loss",
                "Quantile Loss"
            ],
            "correct_answer": 2,
            "explanation": "Log Cosh Loss is very close to the Huber loss in behavior, suggesting it is a differentiable alternative."
        },
        {
            "id": 200,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your model is diagnosed with an optimal solution, what should the model's bias and variance ideally be?",
            "options": [
                "High bias, High variance",
                "Low bias, High variance",
                "Low bias, Low variance",
                "High bias, Low variance"
            ],
            "correct_answer": 2,
            "explanation": "Low bias, low variance - is a good result, just right."
        },
        {
            "id": 201,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "advanced",
            "question": "Which set of unsupervised algorithms is specifically designed to find and group similar items or customers?",
            "options": [
                "Classification and Regression",
                "Clustering and Association",
                "Regularization and Feature Selection",
                "Bias and Variance Reduction"
            ],
            "correct_answer": 1,
            "explanation": "Clustering and Association are tasks under Unsupervised Learning."
        },
        {
            "id": 202,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "advanced",
            "question": "Why is supervised learning more straightforward and easier to understand than unsupervised learning?",
            "options": [
                "Because it is generally more complex.",
                "Because it works with unlabeled data.",
                "Because it has a clear objective of predicting a specific outcome from labeled data.",
                "Because its goal is pattern discovery."
            ],
            "correct_answer": 2,
            "explanation": "Supervised learning is more straightforward and easier to understand, and is defined by having a clear goal of predicting a specific outcome on labeled data."
        },
        {
            "id": 203,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "The conversion from MSLE to RMSLE is primarily done to return the error measurement to what units for easier interpretability?",
            "options": [
                "Squared units.",
                "Logarithmic units.",
                "The original scale of the output variable.",
                "A percentage of the error."
            ],
            "correct_answer": 2,
            "explanation": "The root is taken in RMSLE (Root Mean Squared Logarithmic Error) to bring the error measure back to the original scale, much like RMSE does for MSE."
        },
        {
            "id": 204,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What technique to fight overfitting is implicitly related to stopping the training process before the model becomes too complex?",
            "options": [
                "L1 Regularization",
                "Dropout",
                "Early stopping",
                "Feature Engineering"
            ],
            "correct_answer": 2,
            "explanation": "Early stopping is a regularization method that prevents the model from overfitting by stopping the training process."
        },
        {
            "id": 205,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "Which of the two ensemble methods is considered more difficult to interpret, as its interpretation involves a steeper learning curve?",
            "options": [
                "Single Decision Tree",
                "Random Forest",
                "Linear Regression",
                "None of the above"
            ],
            "correct_answer": 1,
            "explanation": "A random forest is a tad more complicated to interpret, and the learning curve is steep."
        },
        {
            "id": 206,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "If a model's Relative Squared Error (RSE) is 0.01, what can be concluded about its prediction efficacy?",
            "options": [
                "The model is a perfect predictor (RSE=1).",
                "The model performs worse than the simple predictor (RSE>1).",
                "The model performs significantly better than the simple predictor (RSE near 0).",
                "The model is highly biased (RSE near infinity)."
            ],
            "correct_answer": 2,
            "explanation": "The loss index for RSE ranges from 0 to infinity, with 0 corresponding to the ideal. RSE is relative to the error of a simple mean predictor."
        },
        {
            "id": 207,
            "type": "multiple_choice",
            "category": "unsupervised_learning_algorithms",
            "difficulty": "advanced",
            "question": "Which Dimensionality Reduction algorithm is primarily used to transform variables to a new set of uncorrelated components (principal components)?",
            "options": [
                "K-means Clustering",
                "Hierarchical Clustering",
                "Principal Component Analysis (PCA)",
                "Gaussian Mixture Models"
            ],
            "correct_answer": 2,
            "explanation": "Principal Component Analysis (PCA) reduces the dimensionality of the data."
        },
        {
            "id": 208,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "To simplify a model using L2 regularization for linear regression (Ridge regression), what should be done to the L2 coefficient (alpha)?",
            "options": [
                "Decrease it to reduce regularization.",
                "Increase it to increase regularization impact.",
                "Set it to a negative value.",
                "Keep it constant."
            ],
            "correct_answer": 1,
            "explanation": "To simplify the model, you should increase regularization. The alpha for Ridge regression should be increased to increase regularization."
        },
        {
            "id": 209,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "advanced",
            "question": "If you are in a purely exploratory phase of data analysis, not sure what patterns exist, which type of learning should you choose?",
            "options": [
                "Supervised Learning",
                "Reinforcement Learning",
                "Unsupervised Learning",
                "Classification"
            ],
            "correct_answer": 2,
            "explanation": "Unsupervised Learning is recommended when you're not sure what you're looking for and want the algorithm to find natural structures in the data."
        },
        {
            "id": 210,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your training and validation errors are both large, what is the best strategy to immediately start improving the model?",
            "options": [
                "Decrease regularization and simplify the model.",
                "Increase regularization and add more data.",
                "Decrease the quantity of features and simplify the model.",
                "Decrease regularization and try a more powerful/complex model."
            ],
            "correct_answer": 3,
            "explanation": "Large train and val/test errors indicate Underfitting (High Bias). The fix is to complicate the model (less regularization and more complex model/features)."
        },
        {
            "id": 211,
            "type": "multiple_choice",
            "category": "feature_engineering",
            "difficulty": "advanced",
            "question": "Creating a new feature 'area' from existing features 'length' and 'width' is an example of what type of non-linear data transformation?",
            "options": [
                "Dimensionality Reduction",
                "Feature Selection",
                "Feature Engineering",
                "Regularization"
            ],
            "correct_answer": 2,
            "explanation": "Transformation from length and width to area is an example of feature engineering."
        },
        {
            "id": 212,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "What is the key behavioral similarity between Huber Loss and Log Cosh Loss?",
            "options": [
                "They are both non-differentiable at zero.",
                "They are both a combination of linear behavior for large errors and quadratic behavior for small errors.",
                "They are both highly sensitive to outliers.",
                "They are both scale dependent."
            ],
            "correct_answer": 1,
            "explanation": "Log cosh loss is very close to the Huber loss in behaviour as it has linear behaviour for very large values of error and quadratic for small loss values."
        },
        {
            "id": 213,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "Why is a single Decision Tree considered a 'weak predictor'?",
            "options": [
                "Because it is too slow to build.",
                "Because it requires a large number of features.",
                "Because it is susceptible to high variance and overfitting to noise.",
                "Because it is too complicated to interpret."
            ],
            "correct_answer": 2,
            "explanation": "A single decision tree is a weak predictor, primarily due to high variance and susceptibility to overfitting."
        },
        {
            "id": 214,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "The core function of the parameter $\\gamma$ (gamma) in Quantile Loss is to perform what action regarding the model's errors?",
            "options": [
                "Set the non-differentiable threshold.",
                "Normalize the loss against the mean.",
                "Choose how to balance the punishment for positive and negative errors.",
                "Apply the square root to the error."
            ],
            "correct_answer": 2,
            "explanation": "The quantile values are chosen based on how we wish to balance the positive and the negative errors."
        },
        {
            "id": 215,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "What is the key principle to remember when adding new features to a model?",
            "options": [
                "Adding new features always simplifies the model.",
                "Adding new features always complicates the model.",
                "Feature selection is only for neural networks.",
                "Removing all features is the best simplification."
            ],
            "correct_answer": 1,
            "explanation": "Adding new features also complicates the model."
        },
        {
            "id": 216,
            "type": "multiple_choice",
            "category": "supervised_unsupervised_comparison",
            "difficulty": "advanced",
            "question": "If a classification model has high accuracy and reliability, but training is expensive due to the need for labeled data, what type of learning is it likely using?",
            "options": [
                "Unsupervised Learning",
                "Reinforcement Learning",
                "Supervised Learning",
                "Clustering"
            ],
            "correct_answer": 2,
            "explanation": "Supervised learning has the pros of high accuracy/reliability when data is abundant but the con of requiring labeled data, which is time-consuming and expensive."
        },
        {
            "id": 217,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "Since MSLE is scale independent (log of the ratio of values), how does the size of the actual value $y_i$ affect the penalty for a given percentage error?",
            "options": [
                "The penalty is independent of $y_i$.",
                "The penalty is much larger for small $y_i$ errors.",
                "The penalty is consistent for the same percentage error regardless of $y_i$.",
                "The penalty is only linear for large $y_i$."
            ],
            "correct_answer": 2,
            "explanation": "The loss is scale independent as it is the log of the ratio of the values, meaning the penalty is consistent for a proportional error."
        },
        {
            "id": 218,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If the model's complexity is increased to fight underfitting, but the model starts to overfit, what technique should be applied immediately to simplify it?",
            "options": [
                "Decrease regularization (e.g., reduce L1/L2 impact).",
                "Increase the quantity of features (complicate model).",
                "Increase regularization (e.g., increase L1/L2 impact).",
                "Add more layers (complicate model)."
            ],
            "correct_answer": 2,
            "explanation": "To fix overfitting, you should simplify the model by using more regularization."
        },
        {
            "id": 219,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "In Huber Loss, the behavior of the loss function is quadratic for error values that are what size relative to the threshold $\\delta$?",
            "options": [
                "Smaller than $\\delta$ (within the threshold).",
                "Larger than $\\delta$ (beyond the threshold).",
                "Equal to $\\delta$.",
                "Any size, as it's always quadratic."
            ],
            "correct_answer": 0,
            "explanation": "The loss is quadratic if the absolute error is less than $\\delta$ (within the threshold), and linear otherwise."
        },
        {
            "id": 220,
            "type": "multiple_choice",
            "category": "regression_loss_characteristics",
            "difficulty": "advanced",
            "question": "In Huber Loss, the behavior of the loss function is linear for error values that are what size relative to the threshold $\\delta$?",
            "options": [
                "Smaller than $\\delta$ (within the threshold).",
                "Larger than or equal to $\\delta$ (beyond the threshold).",
                "Equal to $\\delta$.",
                "Any size, as it's always linear."
            ],
            "correct_answer": 1,
            "explanation": "The loss is linear for loss values beyond the threshold, and quadratic otherwise."
        },
        {
            "id": 221,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "If your training error is small, but your test error is very large, and the model is not TOO complex, what is the best strategy to improve performance?",
            "options": [
                "Increase regularization (and potentially get more data).",
                "Decrease regularization and increase model complexity.",
                "Increase the quantity of features and decrease data size.",
                "Remove all regularization and use a simpler model."
            ],
            "correct_answer": 0,
            "explanation": "Small training error and large test error indicates Overfitting. The strategies are More regularization and More data."
        },
        {
            "id": 222,
            "type": "multiple_choice",
            "category": "ensemble_methods_comparison",
            "difficulty": "advanced",
            "question": "Which of the two ensemble methods is considered more difficult to interpret, even after the learning curve for interpretation is mastered?",
            "options": [
                "Single Decision Tree",
                "Random Forest",
                "Linear Regression",
                "None of the above"
            ],
            "correct_answer": 1,
            "explanation": "A random forest is a tad more complicated to interpret, and the learning curve is steep."
        },
        {
            "id": 223,
            "type": "multiple_choice",
            "category": "model_improvement_techniques",
            "difficulty": "advanced",
            "question": "When training a model, why is it vital to perform hours of analysis to correctly diagnose underfitting or overfitting before applying a fix?",
            "options": [
                "To ensure all features are correlated.",
                "Because misdiagnosis can lead to spending days/weeks on empty work, applying opposite fixes.",
                "Because all models must be complex.",
                "To maximize the training error."
            ],
            "correct_answer": 1,
            "explanation": "Misdiagnosis can lead to spending days and weeks of work on empty effort, as you apply diametrically opposite actions."
        },
        {
            "id": 224,
            "type": "multiple_choice",
            "category": "feature_engineering",
            "difficulty": "advanced",
            "question": "If a continuous data feature exhibits a heavy right tail (skewed data), what specific transformation is suggested as a feature engineering technique?",
            "options": [
                "Polynomial features",
                "log(x)",
                "ln(|x|+1)",
                "Categorical transformation"
            ],
            "correct_answer": 2,
            "explanation": "ln(|x|+1) is a suggested transformation for data with heavy right tail."
        },
        {
            "id": 225,
            "type": "multiple_choice",
            "category": "regression_loss_evaluation",
            "difficulty": "advanced",
            "question": "If a model's error is measured by Mean Absolute Percentage Error (MAPE), a large MAPE value indicates what about the forecast system?",
            "options": [
                "The accuracy is perfect (MAPE=0).",
                "The accuracy is high (MAPE is small).",
                "The accuracy is low.",
                "The bias is zero."
            ],
            "correct_answer": 2,
            "explanation": "MAPE is a metric used to assess the accuracy of a forecast system. A large error value implies low accuracy."
        }
    ]
}